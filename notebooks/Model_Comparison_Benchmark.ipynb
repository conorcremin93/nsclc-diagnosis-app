{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c28991b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: test_gguf_model_comparison.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from llama_cpp import Llama\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Step 1: Setup ===\n",
    "MODEL_DIR = \"../models/mistral-gguf\"\n",
    "OUTPUT_PATH = \"../outputs/nsclc_model_comparison_results.csv\"\n",
    "gguf_files = [f for f in os.listdir(MODEL_DIR) if f.endswith(\".gguf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d46062e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 2: Dummy Data ===\n",
    "patients = [\n",
    "    {\"PatientID\": \"P001\", \"Text\": \"CT shows solid mass in right lung, biopsy confirms adenocarcinoma.\"},\n",
    "    {\"PatientID\": \"P002\", \"Text\": \"Mild inflammation noted. No solid tumors. Awaiting PET scan.\"},\n",
    "    {\"PatientID\": \"P003\", \"Text\": \"Progressive cough and weight loss. Chest X-ray suspicious for malignancy.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63ed478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 3: Prompt Template ===\n",
    "def build_prompt(text):\n",
    "    return f\"\"\"\n",
    "You are a clinical language model. A patient medical report is provided below. Based on the full clinical context, classify the likelihood that the patient has non-small cell lung cancer (NSCLC) as one of:\n",
    "\n",
    "- Definite\n",
    "- Likely\n",
    "- Unlikely\n",
    "- Uncertain\n",
    "\n",
    "Also provide a brief justification.\n",
    "\n",
    "Patient report:\n",
    "{text}\n",
    "\n",
    "Respond in the following JSON format:\n",
    "{{\"NSCLC_Status\": \"\", \"Justification\": \"\"}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6335024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing models:   0%|          | 0/12 [00:00<?, ?it/s]llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models/mistral-gguf\\mistral-7b-instruct-v0.1.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 2.87 GiB (3.41 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  2939.57 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '10', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: None\n",
      "\n",
      "llama_print_timings:        load time =   32994.53 ms\n",
      "llama_print_timings:      sample time =      19.97 ms /    42 runs   (    0.48 ms per token,  2103.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =   32990.20 ms /   132 tokens (  249.93 ms per token,     4.00 tokens per second)\n",
      "llama_print_timings:        eval time =   14150.84 ms /    41 runs   (  345.14 ms per token,     2.90 tokens per second)\n",
      "llama_print_timings:       total time =   47408.74 ms /   173 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   32994.53 ms\n",
      "llama_print_timings:      sample time =      24.36 ms /    43 runs   (    0.57 ms per token,  1765.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11904.61 ms /    45 tokens (  264.55 ms per token,     3.78 tokens per second)\n",
      "llama_print_timings:        eval time =   16894.27 ms /    42 runs   (  402.24 ms per token,     2.49 tokens per second)\n",
      "llama_print_timings:       total time =   29070.98 ms /    87 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   32994.53 ms\n",
      "llama_print_timings:      sample time =      35.32 ms /    65 runs   (    0.54 ms per token,  1840.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12334.68 ms /    44 tokens (  280.33 ms per token,     3.57 tokens per second)\n",
      "llama_print_timings:        eval time =   25659.40 ms /    64 runs   (  400.93 ms per token,     2.49 tokens per second)\n",
      "llama_print_timings:       total time =   38419.28 ms /   108 tokens\n",
      "Testing models:   8%|▊         | 1/12 [01:55<21:14, 115.90s/it]llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models/mistral-gguf\\mistral-7b-instruct-v0.1.Q3_K_L.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 13\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q3_K:  129 tensors\n",
      "llama_model_loader: - type q5_K:   96 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Large\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.22 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3644.27 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '13', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: None\n",
      "\n",
      "llama_print_timings:        load time =   42074.39 ms\n",
      "llama_print_timings:      sample time =      56.67 ms /    94 runs   (    0.60 ms per token,  1658.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =   42074.18 ms /   132 tokens (  318.74 ms per token,     3.14 tokens per second)\n",
      "llama_print_timings:        eval time =   60169.30 ms /    93 runs   (  646.98 ms per token,     1.55 tokens per second)\n",
      "llama_print_timings:       total time =  103074.33 ms /   225 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   42074.39 ms\n",
      "llama_print_timings:      sample time =      34.32 ms /    55 runs   (    0.62 ms per token,  1602.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14273.04 ms /    45 tokens (  317.18 ms per token,     3.15 tokens per second)\n",
      "llama_print_timings:        eval time =   32781.11 ms /    54 runs   (  607.06 ms per token,     1.65 tokens per second)\n",
      "llama_print_timings:       total time =   47502.87 ms /    99 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   42074.39 ms\n",
      "llama_print_timings:      sample time =      53.65 ms /    80 runs   (    0.67 ms per token,  1491.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14093.41 ms /    44 tokens (  320.30 ms per token,     3.12 tokens per second)\n",
      "llama_print_timings:        eval time =   89362.60 ms /    79 runs   ( 1131.17 ms per token,     0.88 tokens per second)\n",
      "llama_print_timings:       total time =  104511.57 ms /   123 tokens\n",
      "Testing models:  17%|█▋        | 2/12 [06:12<33:08, 198.85s/it]llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models/mistral-gguf\\mistral-7b-instruct-v0.1.Q3_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 12\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q3_K:  129 tensors\n",
      "llama_model_loader: - type q4_K:   92 tensors\n",
      "llama_model_loader: - type q5_K:    4 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 3.28 GiB (3.89 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3355.27 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '12', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: None\n",
      "\n",
      "llama_print_timings:        load time =   44308.53 ms\n",
      "llama_print_timings:      sample time =      61.63 ms /    94 runs   (    0.66 ms per token,  1525.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =   44307.97 ms /   132 tokens (  335.67 ms per token,     2.98 tokens per second)\n",
      "llama_print_timings:        eval time =   54656.76 ms /    93 runs   (  587.71 ms per token,     1.70 tokens per second)\n",
      "llama_print_timings:       total time =   99791.70 ms /   225 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   44308.53 ms\n",
      "llama_print_timings:      sample time =      45.88 ms /    73 runs   (    0.63 ms per token,  1591.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14598.35 ms /    45 tokens (  324.41 ms per token,     3.08 tokens per second)\n",
      "llama_print_timings:        eval time =   36895.67 ms /    72 runs   (  512.44 ms per token,     1.95 tokens per second)\n",
      "llama_print_timings:       total time =   52151.88 ms /   117 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   44308.53 ms\n",
      "llama_print_timings:      sample time =      51.16 ms /    80 runs   (    0.64 ms per token,  1563.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12585.56 ms /    44 tokens (  286.04 ms per token,     3.50 tokens per second)\n",
      "llama_print_timings:        eval time =   34208.99 ms /    79 runs   (  433.03 ms per token,     2.31 tokens per second)\n",
      "llama_print_timings:       total time =   47437.74 ms /   123 tokens\n",
      "Testing models:  25%|██▌       | 3/12 [09:34<30:03, 200.36s/it]llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models/mistral-gguf\\mistral-7b-instruct-v0.1.Q3_K_S.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 11\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q3_K:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Small\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 2.95 GiB (3.50 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3017.27 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '11', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: None\n",
      "\n",
      "llama_print_timings:        load time =   43984.86 ms\n",
      "llama_print_timings:      sample time =      25.52 ms /    42 runs   (    0.61 ms per token,  1645.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =   43984.36 ms /   132 tokens (  333.21 ms per token,     3.00 tokens per second)\n",
      "llama_print_timings:        eval time =   17711.52 ms /    41 runs   (  431.99 ms per token,     2.31 tokens per second)\n",
      "llama_print_timings:       total time =   62006.40 ms /   173 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   43984.86 ms\n",
      "llama_print_timings:      sample time =      50.28 ms /    74 runs   (    0.68 ms per token,  1471.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20664.59 ms /    45 tokens (  459.21 ms per token,     2.18 tokens per second)\n",
      "llama_print_timings:        eval time =   39852.10 ms /    73 runs   (  545.92 ms per token,     1.83 tokens per second)\n",
      "llama_print_timings:       total time =   61208.16 ms /   118 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   43984.86 ms\n",
      "llama_print_timings:      sample time =      51.79 ms /    80 runs   (    0.65 ms per token,  1544.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15103.49 ms /    44 tokens (  343.26 ms per token,     2.91 tokens per second)\n",
      "llama_print_timings:        eval time =   45282.12 ms /    79 runs   (  573.19 ms per token,     1.74 tokens per second)\n",
      "llama_print_timings:       total time =   61086.76 ms /   123 tokens\n",
      "Testing models:  33%|███▎      | 4/12 [12:41<25:59, 194.89s/it]llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models/mistral-gguf\\mistral-7b-instruct-v0.1.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3917.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '2', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: None\n",
      "\n",
      "llama_print_timings:        load time =   52332.96 ms\n",
      "llama_print_timings:      sample time =      52.15 ms /    79 runs   (    0.66 ms per token,  1514.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =   52332.56 ms /   132 tokens (  396.46 ms per token,     2.52 tokens per second)\n",
      "llama_print_timings:        eval time =   88083.23 ms /    78 runs   ( 1129.27 ms per token,     0.89 tokens per second)\n",
      "llama_print_timings:       total time =  141547.40 ms /   210 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   52332.96 ms\n",
      "llama_print_timings:      sample time =      61.46 ms /    87 runs   (    0.71 ms per token,  1415.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19194.77 ms /    45 tokens (  426.55 ms per token,     2.34 tokens per second)\n",
      "llama_print_timings:        eval time =   86444.39 ms /    86 runs   ( 1005.17 ms per token,     0.99 tokens per second)\n",
      "llama_print_timings:       total time =  107185.09 ms /   131 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   52332.96 ms\n",
      "llama_print_timings:      sample time =      58.19 ms /    86 runs   (    0.68 ms per token,  1477.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19438.41 ms /    44 tokens (  441.78 ms per token,     2.26 tokens per second)\n",
      "llama_print_timings:        eval time =   65175.58 ms /    85 runs   (  766.77 ms per token,     1.30 tokens per second)\n",
      "llama_print_timings:       total time =   86164.00 ms /   129 tokens\n",
      "Testing models:  42%|████▏     | 5/12 [18:18<28:43, 246.27s/it]llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models/mistral-gguf\\mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: None\n",
      "\n",
      "llama_print_timings:        load time =   38472.38 ms\n",
      "llama_print_timings:      sample time =      39.82 ms /    55 runs   (    0.72 ms per token,  1381.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   38471.79 ms /   132 tokens (  291.45 ms per token,     3.43 tokens per second)\n",
      "llama_print_timings:        eval time =   56328.28 ms /    54 runs   ( 1043.12 ms per token,     0.96 tokens per second)\n",
      "llama_print_timings:       total time =   95445.77 ms /   186 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   38472.38 ms\n",
      "llama_print_timings:      sample time =      62.46 ms /    94 runs   (    0.66 ms per token,  1504.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12617.70 ms /    45 tokens (  280.39 ms per token,     3.57 tokens per second)\n",
      "llama_print_timings:        eval time =   63029.82 ms /    93 runs   (  677.74 ms per token,     1.48 tokens per second)\n",
      "llama_print_timings:       total time =   77240.42 ms /   138 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   38472.38 ms\n",
      "llama_print_timings:      sample time =      59.30 ms /    85 runs   (    0.70 ms per token,  1433.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12338.85 ms /    44 tokens (  280.43 ms per token,     3.57 tokens per second)\n",
      "llama_print_timings:        eval time =  105919.22 ms /    84 runs   ( 1260.94 ms per token,     0.79 tokens per second)\n",
      "llama_print_timings:       total time =  119611.92 ms /   128 tokens\n",
      "Testing models:  50%|█████     | 6/12 [23:14<26:18, 263.01s/it]llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models/mistral-gguf\\mistral-7b-instruct-v0.1.Q4_K_S.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  217 tensors\n",
      "llama_model_loader: - type q5_K:    8 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 3.86 GiB (4.57 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3947.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '14', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: None\n",
      "\n",
      "llama_print_timings:        load time =   45443.76 ms\n",
      "llama_print_timings:      sample time =      37.66 ms /    55 runs   (    0.68 ms per token,  1460.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45443.10 ms /   132 tokens (  344.27 ms per token,     2.90 tokens per second)\n",
      "llama_print_timings:        eval time =   50653.32 ms /    54 runs   (  938.02 ms per token,     1.07 tokens per second)\n",
      "llama_print_timings:       total time =   96799.83 ms /   186 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   45443.76 ms\n",
      "llama_print_timings:      sample time =      49.89 ms /    70 runs   (    0.71 ms per token,  1403.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12963.01 ms /    45 tokens (  288.07 ms per token,     3.47 tokens per second)\n",
      "llama_print_timings:        eval time =   90221.50 ms /    69 runs   ( 1307.56 ms per token,     0.76 tokens per second)\n",
      "llama_print_timings:       total time =  104164.60 ms /   114 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   45443.76 ms\n",
      "llama_print_timings:      sample time =      71.09 ms /    85 runs   (    0.84 ms per token,  1195.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14403.97 ms /    44 tokens (  327.36 ms per token,     3.05 tokens per second)\n",
      "llama_print_timings:        eval time =   98869.05 ms /    84 runs   ( 1177.01 ms per token,     0.85 tokens per second)\n",
      "llama_print_timings:       total time =  115395.20 ms /   128 tokens\n",
      "Testing models:  58%|█████▊    | 7/12 [28:33<23:26, 281.36s/it]llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models/mistral-gguf\\mistral-7b-instruct-v0.1.Q5_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 8\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.65 GiB (5.52 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4765.49 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '8', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: None\n",
      "\n",
      "llama_print_timings:        load time =   65100.16 ms\n",
      "llama_print_timings:      sample time =      95.73 ms /    55 runs   (    1.74 ms per token,   574.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =   65099.05 ms /   132 tokens (  493.17 ms per token,     2.03 tokens per second)\n",
      "llama_print_timings:        eval time =  427511.59 ms /    54 runs   ( 7916.88 ms per token,     0.13 tokens per second)\n",
      "llama_print_timings:       total time =  495405.73 ms /   186 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   65100.16 ms\n",
      "llama_print_timings:      sample time =      81.62 ms /    70 runs   (    1.17 ms per token,   857.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   30909.41 ms /    45 tokens (  686.88 ms per token,     1.46 tokens per second)\n",
      "llama_print_timings:        eval time =  250723.23 ms /    69 runs   ( 3633.67 ms per token,     0.28 tokens per second)\n",
      "llama_print_timings:       total time =  284530.34 ms /   114 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   65100.16 ms\n",
      "llama_print_timings:      sample time =     131.35 ms /    86 runs   (    1.53 ms per token,   654.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19840.62 ms /    44 tokens (  450.92 ms per token,     2.22 tokens per second)\n",
      "llama_print_timings:        eval time =  537992.84 ms /    85 runs   ( 6329.33 ms per token,     0.16 tokens per second)\n",
      "llama_print_timings:       total time =  562311.22 ms /   129 tokens\n",
      "Testing models:  67%|██████▋   | 8/12 [50:58<41:20, 620.07s/it]llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models/mistral-gguf\\mistral-7b-instruct-v0.1.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4892.99 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '17', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: None\n",
      "\n",
      "llama_print_timings:        load time =   49734.14 ms\n",
      "llama_print_timings:      sample time =      70.78 ms /    55 runs   (    1.29 ms per token,   777.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =   49732.73 ms /   132 tokens (  376.76 ms per token,     2.65 tokens per second)\n",
      "llama_print_timings:        eval time =  371262.21 ms /    54 runs   ( 6875.23 ms per token,     0.15 tokens per second)\n",
      "llama_print_timings:       total time =  422794.04 ms /   186 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   49734.14 ms\n",
      "llama_print_timings:      sample time =      69.47 ms /    85 runs   (    0.82 ms per token,  1223.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =   24765.48 ms /    45 tokens (  550.34 ms per token,     1.82 tokens per second)\n",
      "llama_print_timings:        eval time =  314227.88 ms /    84 runs   ( 3740.81 ms per token,     0.27 tokens per second)\n",
      "llama_print_timings:       total time =  340952.39 ms /   129 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   49734.14 ms\n",
      "llama_print_timings:      sample time =      60.30 ms /    87 runs   (    0.69 ms per token,  1442.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15557.11 ms /    44 tokens (  353.57 ms per token,     2.83 tokens per second)\n",
      "llama_print_timings:        eval time =  235049.30 ms /    86 runs   ( 2733.13 ms per token,     0.37 tokens per second)\n",
      "llama_print_timings:       total time =  254091.51 ms /   130 tokens\n",
      "Testing models:  75%|███████▌  | 9/12 [1:08:02<37:18, 746.14s/it]llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models/mistral-gguf\\mistral-7b-instruct-v0.1.Q5_K_S.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 16\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Small\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.65 GiB (5.52 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4765.49 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '16', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: None\n",
      "\n",
      "llama_print_timings:        load time =   45799.19 ms\n",
      "llama_print_timings:      sample time =      30.10 ms /    55 runs   (    0.55 ms per token,  1827.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45798.99 ms /   132 tokens (  346.96 ms per token,     2.88 tokens per second)\n",
      "llama_print_timings:        eval time =   46535.33 ms /    54 runs   (  861.77 ms per token,     1.16 tokens per second)\n",
      "llama_print_timings:       total time =   93358.19 ms /   186 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   45799.19 ms\n",
      "llama_print_timings:      sample time =      49.30 ms /    85 runs   (    0.58 ms per token,  1724.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15425.49 ms /    45 tokens (  342.79 ms per token,     2.92 tokens per second)\n",
      "llama_print_timings:        eval time =   67777.94 ms /    84 runs   (  806.88 ms per token,     1.24 tokens per second)\n",
      "llama_print_timings:       total time =   84033.26 ms /   129 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   45799.19 ms\n",
      "llama_print_timings:      sample time =      61.46 ms /    87 runs   (    0.71 ms per token,  1415.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17370.09 ms /    44 tokens (  394.77 ms per token,     2.53 tokens per second)\n",
      "llama_print_timings:        eval time =  153347.08 ms /    86 runs   ( 1783.11 ms per token,     0.56 tokens per second)\n",
      "llama_print_timings:       total time =  175117.33 ms /   130 tokens\n",
      "Testing models:  83%|████████▎ | 10/12 [1:13:57<20:50, 625.38s/it]llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models/mistral-gguf\\mistral-7b-instruct-v0.1.Q6_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 5.53 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  5666.09 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '18', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: None\n",
      "\n",
      "llama_print_timings:        load time =   45255.58 ms\n",
      "llama_print_timings:      sample time =     118.91 ms /    94 runs   (    1.27 ms per token,   790.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45254.54 ms /   132 tokens (  342.84 ms per token,     2.92 tokens per second)\n",
      "llama_print_timings:        eval time =  730132.26 ms /    93 runs   ( 7850.88 ms per token,     0.13 tokens per second)\n",
      "llama_print_timings:       total time =  778665.68 ms /   225 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   45255.58 ms\n",
      "llama_print_timings:      sample time =      90.33 ms /    81 runs   (    1.12 ms per token,   896.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20513.50 ms /    45 tokens (  455.86 ms per token,     2.19 tokens per second)\n",
      "llama_print_timings:        eval time =  624328.27 ms /    80 runs   ( 7804.10 ms per token,     0.13 tokens per second)\n",
      "llama_print_timings:       total time =  647603.43 ms /   125 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   45255.58 ms\n",
      "llama_print_timings:      sample time =     129.41 ms /    94 runs   (    1.38 ms per token,   726.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18959.45 ms /    44 tokens (  430.90 ms per token,     2.32 tokens per second)\n",
      "llama_print_timings:        eval time =  776867.06 ms /    93 runs   ( 8353.41 ms per token,     0.12 tokens per second)\n",
      "llama_print_timings:       total time =  799875.57 ms /   137 tokens\n",
      "Testing models:  92%|█████████▏| 11/12 [1:51:05<18:36, 1116.01s/it]llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from ../models/mistral-gguf\\mistral-7b-instruct-v0.1.Q8_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 7.17 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  7338.64 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '7', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: None\n",
      "\n",
      "llama_print_timings:        load time =   41766.19 ms\n",
      "llama_print_timings:      sample time =      73.11 ms /    55 runs   (    1.33 ms per token,   752.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =   41765.12 ms /   132 tokens (  316.40 ms per token,     3.16 tokens per second)\n",
      "llama_print_timings:        eval time =  526998.29 ms /    54 runs   ( 9759.23 ms per token,     0.10 tokens per second)\n",
      "llama_print_timings:       total time =  570864.68 ms /   186 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   41766.19 ms\n",
      "llama_print_timings:      sample time =     123.27 ms /    94 runs   (    1.31 ms per token,   762.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21507.01 ms /    45 tokens (  477.93 ms per token,     2.09 tokens per second)\n",
      "llama_print_timings:        eval time =  932868.30 ms /    93 runs   (10030.84 ms per token,     0.10 tokens per second)\n",
      "llama_print_timings:       total time =  958162.90 ms /   138 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   41766.19 ms\n",
      "llama_print_timings:      sample time =     117.21 ms /    85 runs   (    1.38 ms per token,   725.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21316.37 ms /    44 tokens (  484.46 ms per token,     2.06 tokens per second)\n",
      "llama_print_timings:        eval time =  847287.53 ms /    84 runs   (10086.76 ms per token,     0.10 tokens per second)\n",
      "llama_print_timings:       total time =  872163.32 ms /   128 tokens\n",
      "Testing models: 100%|██████████| 12/12 [2:31:09<00:00, 755.78s/it] \n"
     ]
    }
   ],
   "source": [
    "# === Step 4: Run each model and collect results ===\n",
    "results = []\n",
    "\n",
    "for model_file in tqdm(gguf_files, desc=\"Testing models\"):\n",
    "    model_path = os.path.join(MODEL_DIR, model_file)\n",
    "    try:\n",
    "        llm = Llama(model_path=model_path, n_ctx=2048, n_threads=6)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load model {model_file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    for patient in patients:\n",
    "        prompt = build_prompt(patient[\"Text\"])\n",
    "        response = llm(prompt, max_tokens=512, temperature=0.0, top_p=0.9)\n",
    "        full_text = response[\"choices\"][0][\"text\"]\n",
    "\n",
    "        # Try to extract the JSON object from model output\n",
    "        json_pattern = r\"\\{.*?\\}\"\n",
    "        match = re.search(json_pattern, full_text, re.DOTALL)\n",
    "\n",
    "        if match:\n",
    "            try:\n",
    "                parsed = json.loads(match.group(0))\n",
    "                nsclc_status = parsed.get(\"NSCLC_Status\", \"\")\n",
    "                justification = parsed.get(\"Justification\", \"\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                nsclc_status = \"ParseError\"\n",
    "                justification = f\"JSON error after match: {e}\\nRaw output: {match.group(0)}\"\n",
    "        else:\n",
    "            nsclc_status = \"ParseError\"\n",
    "            justification = f\"No JSON found. Raw output: {full_text.strip()}\"\n",
    "\n",
    "        results.append({\n",
    "            \"Model\": model_file,\n",
    "            \"PatientID\": patient[\"PatientID\"],\n",
    "            \"NSCLC_Status\": nsclc_status,\n",
    "            \"Justification\": justification\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b5cfa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: ../outputs/nsclc_model_comparison_results.csv\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Step 5: Save results ===\n",
    "df_results = pd.DataFrame(results)\n",
    "os.makedirs(\"../outputs\", exist_ok=True)\n",
    "df_results.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Results saved to: {OUTPUT_PATH}\")\n",
    "\n",
    "# === Step 6: Plot success rate ===\n",
    "summary = df_results.groupby(\"Model\")[\"NSCLC_Status\"].apply(lambda x: (x != \"ParseError\").sum())\n",
    "summary.plot(kind=\"bar\", title=\"Number of Successful Parses per Model\", figsize=(12, 6))\n",
    "plt.ylabel(\"Successful Parses (out of 3)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/model_parse_success.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
